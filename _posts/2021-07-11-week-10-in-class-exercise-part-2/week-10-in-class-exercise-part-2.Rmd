---
title: "Week 10 In-Class Exercise 2"
description: |
  Today I learnt in class: How to use different R packages to visualize text using different methods such as tag cloud and word tree. 
author:
  - name: Kelly Koh
    url: https://www.linkedin.com/in/kellykkw/
    affiliation: School of Computing and Information Systems, Singapore Management University
    affiliation_url: https://scis.smu.edu.sg/
date: 07-11-2021
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    toc_float: true
    self_contained: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

## Install packages and loading data

### Install the necessary packages if they are not in library
```{r echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
packages = c('tidyverse','DT',
             # Handling time variables
             'lubridate','hms',
             # Text analytics
             'tidytext','tidyverse','widyr','wordcloud',
             'ggwordcloud','textplot','igraph','ggraph')

for (p in packages) {
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

## Data cleaning

### Import data from csv and preview data
```{r echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
data_2001 <- read_csv("data/csv-2001-2131.csv")

# Create a folder list
news20 <- "data/20news/"

# Define function to read all files from a folder into a data frame
read_folder <- function(infolder) {
    tibble(file = dir(infolder,
                      full.names = TRUE)) %>%
    mutate(text = map(file,
                      read_lines)) %>%
    transmute(id = basename(file),
                   text) %>%
    unnest(text)
}

# Read in all the messages from the 20news folder
raw_text <- tibble(folder =
                     dir(news20,
                         full.names = TRUE)) %>%
            # map is part of the tidy R package
            mutate(folder_out = map(folder,
                                    read_folder)) %>%
            unnest(cols = c(folder_out)) %>%
            transmute(newsgroup = basename(folder),
                      id, text)

write_rds(raw_text,"data/rds/news20.rds")
```


### Checking if the data loaded is correct
```{r echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
```


### Cleaning text data
```{r echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
cleaned_text <- raw_text %>%
                group_by(newsgroup, id) %>%
                # Refer to regular expressions to indicate patterns
                filter(cumsum(text == "") > 0, 
                       cumsum(str_detect(text, "^--")) == 0) %>%
                ungroup()

cleaned_text <- cleaned_text %>%
                filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
                       | text == "",
                       !str_detect(text,
                                   "writes(:|\\.\\.\\.)$"),
                       !str_detect(text,
                                   "^In article <")
                       )
```

## Text Data Processing

### Tokenization - extract words out of the string of text
```{r echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Split a column into tokens using unnest_tokens
# Remove stop-words e.g.“the”, “of”, “to” that are not useful for analysis
usenet_words <- cleaned_text %>%
                unnest_tokens(word,text) %>% 
                filter(str_detect(word, "[a-z']$"),
                       !word %in% stop_words$word)


```


## Visualizing text data

### Visualize the data using cccc
```{r echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# xx

```

